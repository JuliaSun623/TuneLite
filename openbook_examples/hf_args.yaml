# model
model_name_or_path: llama-13B
cache_dir: /remote-home/share/llama/
llama_dir: /remote-home/share/llama/
# data
dataset_name: 'openbookqa'
data_dir: data/
refresh: true
data_tag: 'template0'
prompt_type: 'natural'
train_on_inputs: false
max_length: 256
few_shot_size: 5
# tunelite
tag: 'template0'
max_new_tokens: 100
temperature: 1.0
top_p: 1.0
# trainer
output_dir: outputs/
overwrite_output_dir: true
do_train: true
do_eval: true
evaluation_strategy: epoch
per_device_train_batch_size: 4
per_device_eval_batch_size: 2
gradient_accumulation_steps: 1
num_train_epochs: 15
learning_rate: 0.0001
lr_scheduler_type: 'linear'
warmup: 0.01
clip_grad_value: 1.0
weight_decay: 0
#warmup_ratio: 0.05
logging_steps: 1
optim: 'sgd'
load_best_model_at_end: true
save_strategy: epoch
save_total_limit: 0
predict_with_generate: true
metric_for_best_model: acc
seed: 42
report_to: wandb
dataloader_pin_memory: false
save_on_each_node: true
fp16: true
remove_unused_columns: false
gradient_checkpointing: false